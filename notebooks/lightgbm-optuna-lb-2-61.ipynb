{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bb6d7e2",
   "metadata": {
    "papermill": {
     "duration": 23.137391,
     "end_time": "2024-09-29T17:48:25.157238",
     "exception": false,
     "start_time": "2024-09-29T17:48:02.019847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.5.2\n",
      "Uninstalling scikit-learn-1.5.2:\n",
      "  Successfully uninstalled scikit-learn-1.5.2\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\administrator\\desktop\\raphi_other\\repositories\\ts\\.envs\\dev_env\\lib\\site-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\administrator\\desktop\\raphi_other\\repositories\\ts\\.envs\\dev_env\\lib\\site-packages (from scikit-learn==1.5.2) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\administrator\\desktop\\raphi_other\\repositories\\ts\\.envs\\dev_env\\lib\\site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\administrator\\desktop\\raphi_other\\repositories\\ts\\.envs\\dev_env\\lib\\site-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall scikit-learn -y\n",
    "!pip install scikit-learn==1.5.2\n",
    "# #!pip uninstall pandas -y\n",
    "# #!pip install pandas==2.1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c5cb824",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.942992,
     "end_time": "2024-09-29T17:48:26.107135",
     "exception": false,
     "start_time": "2024-09-29T17:48:25.164143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/blood_glucose/activities.txt\n",
      "../data/raw/blood_glucose/sample_submission.csv\n",
      "../data/raw/blood_glucose/test.csv\n",
      "../data/raw/blood_glucose/train.csv\n",
      "../data/raw/blood_glucose/.ipynb_checkpoints\\activities-checkpoint.txt\n",
      "../data/raw/blood_glucose/.ipynb_checkpoints\\sample_submission-checkpoint.csv\n",
      "../data/raw/blood_glucose/.ipynb_checkpoints\\test-checkpoint.csv\n",
      "../data/raw/blood_glucose/.ipynb_checkpoints\\train-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../data/raw/blood_glucose/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7753c25f",
   "metadata": {
    "papermill": {
     "duration": 16.897385,
     "end_time": "2024-09-29T17:48:43.011332",
     "exception": false,
     "start_time": "2024-09-29T17:48:26.113947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #!pip install optuna\n",
    "# !pip uninstall lightgbm -y\n",
    "# !pip install lightgbm --config-settings=cmake.define.USE_CUDA=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47246d7d",
   "metadata": {
    "papermill": {
     "duration": 2.724164,
     "end_time": "2024-09-29T17:48:45.743477",
     "exception": false,
     "start_time": "2024-09-29T17:48:43.019313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 36\n",
      "[LightGBM] [Info] Number of data points in the train set: 50, number of used features: 2\n",
      "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 620, Vendor: Intel(R) Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 2 dense feature groups (0.00 MB) transferred to GPU in 0.000206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.460000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "import numpy as np\n",
    "def check_gpu_support():\n",
    "    try:\n",
    "        data = np.random.rand(50, 2)\n",
    "        label = np.random.randint(2, size=50)\n",
    "        train_data = lightgbm.Dataset(data, label=label)\n",
    "        params = {'num_iterations': 1, 'device': 'gpu'}\n",
    "        gbm = lightgbm.train(params, train_set=train_data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "check_gpu_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f3885d3",
   "metadata": {
    "papermill": {
     "duration": 0.217921,
     "end_time": "2024-09-29T17:48:45.969954",
     "exception": false,
     "start_time": "2024-09-29T17:48:45.752033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# check xgboost version\n",
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8d32e37",
   "metadata": {
    "papermill": {
     "duration": 0.462078,
     "end_time": "2024-09-29T17:48:46.440517",
     "exception": false,
     "start_time": "2024-09-29T17:48:45.978439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc  # Import garbage collection for manual memory management\n",
    "\n",
    "def remove_columns(df, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified columns from the DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame.\n",
    "    :param columns_to_remove: List of column names to remove.\n",
    "    :return: DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# Function to reduce memory usage by downcasting numerical data types\n",
    "def reduce_memory_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            else:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        else:\n",
    "            # Convert object types to category type for memory efficiency\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to preprocess data and train model with Optuna for hyperparameter tuning\n",
    "def preprocess_and_train_with_optuna(dataset_path, target_variable, columns_to_remove=None, n_trials=3, use_knn_imputer=True, chunk_size=5000):\n",
    "    \n",
    "    # Load the dataset in chunks for memory efficiency\n",
    "    chunks = pd.read_csv(dataset_path, chunksize=chunk_size)\n",
    "\n",
    "    # Concatenate chunks into a single DataFrame while reducing memory usage\n",
    "    data = pd.concat([reduce_memory_usage(chunk) for chunk in chunks])\n",
    "    \n",
    "    if columns_to_remove is not None:\n",
    "        data = remove_columns(data, columns_to_remove)\n",
    "        \n",
    "    # Explicit garbage collection to free up memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Check if target variable exists in dataset\n",
    "    if target_variable not in data.columns:\n",
    "        raise ValueError(f\"{target_variable} not found in the dataset\")\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=[target_variable])\n",
    "    y = data[target_variable]\n",
    "\n",
    "    # Optimize target column by converting to float32\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_features = X.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Intelligent imputation for numerical features\n",
    "    if use_knn_imputer:\n",
    "        # Use KNN Imputer for numerical columns (more advanced, but can be heavy on memory)\n",
    "        numerical_imputer = KNNImputer(n_neighbors=3)\n",
    "    else:\n",
    "        # Use Median Imputer for numerical columns\n",
    "        numerical_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Preprocessing for numerical features (KNN Imputation or Median Imputation and Scaling)\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', numerical_imputer),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Intelligent imputation for categorical features (add 'Missing' as a new category)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine numerical and categorical transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Define hyperparameter search space\n",
    "        xgboost_param = {\n",
    "            'device':'gpu',\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'tree_method': 'gpu_hist',  # Enable GPU for XGBoost\n",
    "            'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n",
    "            'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "        }\n",
    "\n",
    "        # Create LightGBM model\n",
    "        model = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', xg.XGBRegressor(**xgboost_param, random_state=42))\n",
    "        ])\n",
    "        print(\"Training\")\n",
    "        # Cross-validation with 5-fold to evaluate the performance using RMSE\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        rmse = -np.mean(cv_scores)  # RMSE (scikit-learn uses negative RMSE by default)\n",
    "        return rmse\n",
    "\n",
    "    # Create an Optuna study and optimize it\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "   \n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    # Train the final model using the best hyperparameters\n",
    "    best_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', lgb.LGBMRegressor(**best_params, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate and print final performance using RMSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Test RMSE: {rmse}')\n",
    "\n",
    "    # Explicit garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    return best_model, study\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc70c20c-425b-4b7d-9d2a-87db213da1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3756\\475696977.py:1: DtypeWarning: Columns (435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset_path = pd.read_csv(r'../data/raw/blood_glucose/train.csv')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/blood_glucose/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m target_variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbg+1:00\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m best_model, study \u001b[38;5;241m=\u001b[39m preprocess_and_train_with_optuna(dataset_path, target_variable)\n",
      "Cell \u001b[1;32mIn[32], line 44\u001b[0m, in \u001b[0;36mpreprocess_and_train_with_optuna\u001b[1;34m(dataset_path, target_variable, columns_to_remove, n_trials, use_knn_imputer, chunk_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_train_with_optuna\u001b[39m(dataset_path, target_variable, columns_to_remove\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, use_knn_imputer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m):\n\u001b[0;32m     42\u001b[0m     \n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Load the dataset in chunks for memory efficiency\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path, chunksize\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Concatenate chunks into a single DataFrame while reducing memory usage\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([reduce_memory_usage(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:719\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    716\u001b[0m errors \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# read_csv does not know whether the buffer is opened in binary/text mode\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_binary_mode(path_or_buf, mode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    720\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:1181\u001b[0m, in \u001b[0;36m_is_binary_mode\u001b[1;34m(handle, mode)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(handle), text_classes):\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, _get_binary_io_classes()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   1182\u001b[0m     handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\n\u001b[0;32m   1183\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'method' is not iterable"
     ]
    }
   ],
   "source": [
    "dataset_path = pd.read_csv(r'../data/raw/blood_glucose/train.csv')\n",
    "target_variable = 'bg+1:00'\n",
    "\n",
    "best_model, study = preprocess_and_train_with_optuna(dataset_path, target_variable)#, columns_to_remove=None, n_trials=3, use_knn_imputer=True, chunk_size=5000):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcec3372",
   "metadata": {
    "papermill": {
     "duration": 0.020721,
     "end_time": "2024-09-29T17:48:46.470130",
     "exception": false,
     "start_time": "2024-09-29T17:48:46.449409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Predict function that accepts a DataFrame\n",
    "def predict(model, input_data,columns_to_remove = None):\n",
    "    \"\"\"\n",
    "    Predicts target values using the trained model on new input data.\n",
    "\n",
    "    :param model: Trained regression model pipeline\n",
    "    :param input_data: Pandas DataFrame containing input features\n",
    "    :return: Predicted values\n",
    "    \"\"\"\n",
    "    # Ensure input data is in DataFrame format\n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        raise ValueError(\"Input data must be a pandas DataFrame\")\n",
    "    \n",
    "    if columns_to_remove is not None:\n",
    "        data = remove_columns(input_data, columns_to_remove)\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    return predictions     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f21ffa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T17:48:46.490075Z",
     "iopub.status.busy": "2024-09-29T17:48:46.488994Z",
     "iopub.status.idle": "2024-09-29T17:48:46.535707Z",
     "shell.execute_reply": "2024-09-29T17:48:46.534482Z"
    },
    "papermill": {
     "duration": 0.060358,
     "end_time": "2024-09-29T17:48:46.538968",
     "exception": false,
     "start_time": "2024-09-29T17:48:46.478610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/kaggle/input/trainedlightgbm/scikitlearn/default/1/finalmodel.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4462f029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T17:48:46.557416Z",
     "iopub.status.busy": "2024-09-29T17:48:46.556985Z",
     "iopub.status.idle": "2024-09-29T17:48:47.446745Z",
     "shell.execute_reply": "2024-09-29T17:48:47.444799Z"
    },
    "papermill": {
     "duration": 0.903487,
     "end_time": "2024-09-29T17:48:47.450962",
     "exception": false,
     "start_time": "2024-09-29T17:48:46.547475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] lambda_l2 is set with lambda=0.007981650627487073, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007981650627487073\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame for prediction (replace with actual data)\n",
    "test =  pd.read_csv('/kaggle/input/brist1d/test.csv')\n",
    "\n",
    "# Get predictions\n",
    "predictions = predict(model, test,columns_to_remove=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fcf8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T17:48:47.477095Z",
     "iopub.status.busy": "2024-09-29T17:48:47.476462Z",
     "iopub.status.idle": "2024-09-29T17:48:47.490397Z",
     "shell.execute_reply": "2024-09-29T17:48:47.488664Z"
    },
    "papermill": {
     "duration": 0.032333,
     "end_time": "2024-09-29T17:48:47.494442",
     "exception": false,
     "start_time": "2024-09-29T17:48:47.462109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['bg+1:00'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adad9e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T17:48:47.521350Z",
     "iopub.status.busy": "2024-09-29T17:48:47.520732Z",
     "iopub.status.idle": "2024-09-29T17:48:47.533317Z",
     "shell.execute_reply": "2024-09-29T17:48:47.532140Z"
    },
    "papermill": {
     "duration": 0.030974,
     "end_time": "2024-09-29T17:48:47.537105",
     "exception": false,
     "start_time": "2024-09-29T17:48:47.506131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_sub_df = test[['id','bg+1:00']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a531bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T17:48:47.557943Z",
     "iopub.status.busy": "2024-09-29T17:48:47.557478Z",
     "iopub.status.idle": "2024-09-29T17:48:47.580800Z",
     "shell.execute_reply": "2024-09-29T17:48:47.579332Z"
    },
    "papermill": {
     "duration": 0.036643,
     "end_time": "2024-09-29T17:48:47.584175",
     "exception": false,
     "start_time": "2024-09-29T17:48:47.547532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_sub_df.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9553358,
     "sourceId": 82611,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 128179,
     "modelInstanceId": 103967,
     "sourceId": 123522,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.557606,
   "end_time": "2024-09-29T17:48:48.417005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-29T17:47:58.859399",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
